<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8" />
  <title>YOLO ONNX Web推論</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <style>
    body { margin: 0; background: #000; color: #fff; text-align: center; font-family: sans-serif; }
    video, canvas { width: 100vw; height: 100vh; object-fit: cover; }
    #controls {
      position: fixed; bottom: 10px; left: 50%; transform: translateX(-50%);
      background: rgba(0,0,0,0.5); padding: 10px 20px; border-radius: 10px;
    }
    #status {
      position: fixed; left: 10px; bottom: 10px;
      background: rgba(0,0,0,0.5); padding: 5px 10px;
      border-radius: 6px; font-size: 14px; text-align: left;
    }
  </style>
</head>
<body>
  <video id="camera" autoplay playsinline></video>
  <canvas id="canvas"></canvas>

  <div id="controls">
    <input id="idInput" placeholder="モデルID (例: 1234)" />
    <button id="loadBtn">モデル読み込み</button>
    <button id="captureBtn">📸 推論する</button>
  </div>
  <div id="status">待機中</div>

  <script>
    const video = document.getElementById("camera");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");
    const status = document.getElementById("status");
    let session = null;
    let labels = ["object"]; // クラスは1種類前提

    // --- カメラ起動 ---
    async function initCamera() {
      try {
        const devices = await navigator.mediaDevices.enumerateDevices();
        const backCamera = devices.find(d => d.kind === 'videoinput' && d.label.toLowerCase().includes('back'));
        const constraints = backCamera
          ? { video: { deviceId: { exact: backCamera.deviceId } } }
          : { video: { facingMode: "environment" } };

        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;

        return new Promise(resolve => {
          video.onloadedmetadata = () => {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            resolve();
          };
        });
      } catch (err) {
        console.error("カメラ起動失敗:", err);
        status.textContent = "カメラエラー: " + err.message;
      }
    }

    // --- モデル読み込み ---
    document.getElementById("loadBtn").addEventListener("click", async () => {
      const id = document.getElementById("idInput").value.trim();
      if (!id) return alert("IDを入力してください");

      const modelPath = `./${id}.onnx`;
      const jsonPath = `./${id}-k.json`;

      try {
        status.textContent = "モデル読み込み中...";
        console.log("モデル読み込み:", modelPath);

        // GPU対応（WebGPU→WASM自動フォールバック）
        let provider = "webgpu";
        try {
          session = await ort.InferenceSession.create(modelPath, { executionProviders: [provider] });
          status.textContent = "✅ モデルOK (WebGPU)";
        } catch (gpuErr) {
          console.warn("WebGPU失敗、WASMへフォールバック:", gpuErr);
          provider = "wasm";
          session = await ort.InferenceSession.create(modelPath, { executionProviders: [provider] });
          status.textContent = "✅ モデルOK (WASM)";
        }

        console.log(`✅ モデルOK (${provider})`);

        // JSON読み込み
        try {
          const res = await fetch(jsonPath);
          labels = await res.json();
          console.log("✅ json OK", labels);
        } catch (e) {
          console.warn("json読み込み失敗:", e);
        }
      } catch (e) {
        console.error("モデル読み込み失敗:", e);
        status.textContent = "モデル読み込みエラー";
      }
    });

    // --- 推論処理 ---
    document.getElementById("captureBtn").addEventListener("click", async () => {
      if (!session) {
        alert("モデルを先に読み込んでください");
        return;
      }

      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);

      const data = new Float32Array(canvas.width * canvas.height * 3);
      for (let i = 0; i < canvas.width * canvas.height; i++) {
        data[i * 3 + 0] = imageData.data[i * 4 + 0] / 255.0;
        data[i * 3 + 1] = imageData.data[i * 4 + 1] / 255.0;
        data[i * 3 + 2] = imageData.data[i * 4 + 2] / 255.0;
      }

      const tensor = new ort.Tensor("float32", data, [1, 3, canvas.height, canvas.width]);

      try {
        const results = await session.run({ images: tensor });
        const output = results.output0.data;
        console.log("推論結果:", output);

        // --- 結果処理 ---
        const numDetections = output.length / 6; // [x1, y1, x2, y2, score, class]
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        ctx.lineWidth = 3;
        ctx.font = "20px sans-serif";

        for (let i = 0; i < numDetections; i++) {
          const [x1, y1, x2, y2, score, classId] = output.slice(i * 6, i * 6 + 6);
          if (score < 0.3) continue; // スコアが低いものは無視

          const label = labels[Math.floor(classId)] || "object";
          const w = x2 - x1, h = y2 - y1;
          ctx.strokeStyle = "lime";
          ctx.fillStyle = "rgba(0,0,0,0.6)";
          ctx.fillRect(x1, y1 - 30, ctx.measureText(`${label} ${(score * 100).toFixed(1)}%`).width + 10, 30);
          ctx.fillStyle = "lime";
          ctx.fillText(`${label} ${(score * 100).toFixed(1)}%`, x1 + 5, y1 - 10);
          ctx.strokeRect(x1, y1, w, h);
        }

        status.textContent = "推論完了 ✅";
      } catch (e) {
        console.error("推論エラー:", e);
        status.textContent = "推論エラー";
      }
    });

    initCamera();
  </script>
</body>
</html>
