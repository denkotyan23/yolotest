<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8" />
  <title>YOLO ONNX Webæ¨è«–</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <style>
    body { margin: 0; background: #000; color: #fff; text-align: center; font-family: sans-serif; }
    video, canvas { width: 100vw; height: 100vh; object-fit: cover; }
    #controls {
      position: fixed; bottom: 10px; left: 50%; transform: translateX(-50%);
      background: rgba(0,0,0,0.5); padding: 10px 20px; border-radius: 10px;
    }
    #status {
      position: fixed; left: 10px; bottom: 10px;
      background: rgba(0,0,0,0.5); padding: 5px 10px;
      border-radius: 6px; font-size: 14px; text-align: left;
    }
  </style>
</head>
<body>
  <video id="camera" autoplay playsinline></video>
  <canvas id="canvas"></canvas>

  <div id="controls">
    <input id="idInput" placeholder="ãƒ¢ãƒ‡ãƒ«ID (ä¾‹: 1234)" />
    <button id="loadBtn">ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿</button>
    <button id="captureBtn">ğŸ“¸ æ¨è«–ã™ã‚‹</button>
  </div>
  <div id="status">å¾…æ©Ÿä¸­</div>

  <script>
    const video = document.getElementById("camera");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");
    const status = document.getElementById("status");
    let session = null;
    let labels = ["object"]; // ã‚¯ãƒ©ã‚¹ã¯1ç¨®é¡å‰æ

    // --- ã‚«ãƒ¡ãƒ©èµ·å‹• ---
    async function initCamera() {
      try {
        const devices = await navigator.mediaDevices.enumerateDevices();
        const backCamera = devices.find(d => d.kind === 'videoinput' && d.label.toLowerCase().includes('back'));
        const constraints = backCamera
          ? { video: { deviceId: { exact: backCamera.deviceId } } }
          : { video: { facingMode: "environment" } };

        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;

        return new Promise(resolve => {
          video.onloadedmetadata = () => {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            resolve();
          };
        });
      } catch (err) {
        console.error("ã‚«ãƒ¡ãƒ©èµ·å‹•å¤±æ•—:", err);
        status.textContent = "ã‚«ãƒ¡ãƒ©ã‚¨ãƒ©ãƒ¼: " + err.message;
      }
    }

    // --- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ ---
    document.getElementById("loadBtn").addEventListener("click", async () => {
      const id = document.getElementById("idInput").value.trim();
      if (!id) return alert("IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„");

      const modelPath = `./${id}.onnx`;
      const jsonPath = `./${id}-k.json`;

      try {
        status.textContent = "ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...";
        console.log("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿:", modelPath);

        // GPUå¯¾å¿œï¼ˆWebGPUâ†’WASMè‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        let provider = "webgpu";
        try {
          session = await ort.InferenceSession.create(modelPath, { executionProviders: [provider] });
          status.textContent = "âœ… ãƒ¢ãƒ‡ãƒ«OK (WebGPU)";
        } catch (gpuErr) {
          console.warn("WebGPUå¤±æ•—ã€WASMã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯:", gpuErr);
          provider = "wasm";
          session = await ort.InferenceSession.create(modelPath, { executionProviders: [provider] });
          status.textContent = "âœ… ãƒ¢ãƒ‡ãƒ«OK (WASM)";
        }

        console.log(`âœ… ãƒ¢ãƒ‡ãƒ«OK (${provider})`);

        // JSONèª­ã¿è¾¼ã¿
        try {
          const res = await fetch(jsonPath);
          labels = await res.json();
          console.log("âœ… json OK", labels);
        } catch (e) {
          console.warn("jsonèª­ã¿è¾¼ã¿å¤±æ•—:", e);
        }
      } catch (e) {
        console.error("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—:", e);
        status.textContent = "ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼";
      }
    });

    // --- æ¨è«–å‡¦ç† ---
    document.getElementById("captureBtn").addEventListener("click", async () => {
      if (!session) {
        alert("ãƒ¢ãƒ‡ãƒ«ã‚’å…ˆã«èª­ã¿è¾¼ã‚“ã§ãã ã•ã„");
        return;
      }

      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);

      const data = new Float32Array(canvas.width * canvas.height * 3);
      for (let i = 0; i < canvas.width * canvas.height; i++) {
        data[i * 3 + 0] = imageData.data[i * 4 + 0] / 255.0;
        data[i * 3 + 1] = imageData.data[i * 4 + 1] / 255.0;
        data[i * 3 + 2] = imageData.data[i * 4 + 2] / 255.0;
      }

      const tensor = new ort.Tensor("float32", data, [1, 3, canvas.height, canvas.width]);

      try {
        const results = await session.run({ images: tensor });
        const output = results.output0.data;
        console.log("æ¨è«–çµæœ:", output);

        // --- çµæœå‡¦ç† ---
        const numDetections = output.length / 6; // [x1, y1, x2, y2, score, class]
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        ctx.lineWidth = 3;
        ctx.font = "20px sans-serif";

        for (let i = 0; i < numDetections; i++) {
          const [x1, y1, x2, y2, score, classId] = output.slice(i * 6, i * 6 + 6);
          if (score < 0.3) continue; // ã‚¹ã‚³ã‚¢ãŒä½ã„ã‚‚ã®ã¯ç„¡è¦–

          const label = labels[Math.floor(classId)] || "object";
          const w = x2 - x1, h = y2 - y1;
          ctx.strokeStyle = "lime";
          ctx.fillStyle = "rgba(0,0,0,0.6)";
          ctx.fillRect(x1, y1 - 30, ctx.measureText(`${label} ${(score * 100).toFixed(1)}%`).width + 10, 30);
          ctx.fillStyle = "lime";
          ctx.fillText(`${label} ${(score * 100).toFixed(1)}%`, x1 + 5, y1 - 10);
          ctx.strokeRect(x1, y1, w, h);
        }

        status.textContent = "æ¨è«–å®Œäº† âœ…";
      } catch (e) {
        console.error("æ¨è«–ã‚¨ãƒ©ãƒ¼:", e);
        status.textContent = "æ¨è«–ã‚¨ãƒ©ãƒ¼";
      }
    });

    initCamera();
  </script>
</body>
</html>
